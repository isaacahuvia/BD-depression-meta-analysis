---
title: "3_Analysis"
output: html_document
---

```{r, include = F}
knitr::opts_chunk$set(echo = F, warning = F)
```

```{r, message = F, warning = F}
## Startup
rm(list = ls())
library(easypackages)
libraries("tidyverse", "metafor", "magrittr", "knitr", "kableExtra")


## Load Data
df <- readRDS("C:\\Users\\isaac\\Google Drive\\Research\\Projects\\Body Dissatisfaction Meta-Analysis\\BD-depression-meta-analysis\\Data\\Data With Effect Sizes.rds")



## Update Effect Labels
df <- df %>%
  group_by(studyID) %>%
  mutate(t.group.i = match(t.groupDetailed, unique(t.groupDetailed)),
         c.group.i = match(c.groupDetailed, unique(c.groupDetailed)),
         t.group.n = length(unique(t.group.i)),
         c.group.n = length(unique(c.group.i)),
         t.groupNew = paste0("Trt (", t.group.i, ")"),
         c.groupNew = paste0("Ctrl (", c.group.i, ")"),
         t.groupAlt = if_else(t.group.n > 1, paste0("Trt (", t.group.i, ")"), "Trt"),
         c.groupAlt = if_else(c.group.n > 1, paste0("Ctrl (", c.group.i, ")"), "Ctrl")) %>%
  ungroup() %>%
  mutate(effectLabelNew = paste0(t.groupNew, " vs. ", c.groupNew, ", ", outcome),
         effectLabelAlt = paste0(t.groupAlt, " vs. ", c.groupAlt, ", ", outcome))


## Prepare Follow-up Data
#Reshape to long dataset, code for follow-up time-point, and keep only follow-up outcomes
df.fu <- df %>%
  select_at(vars(-matches(".post"))) %>%
  rename(nt.fu1 = t.fu1.n, nt.fu2 = t.fu2.n, nc.fu1 = c.fu1.n, nc.fu2 = c.fu2.n) %>%
  pivot_longer(cols = c(studyCompletion.fu1, studyCompletion.fu2, nt.fu1, nt.fu2, nc.fu1, nc.fu2, time.fu1:varG.fu2_t2),
                     names_to = c("var", "followUp"),
                     names_sep = "\\.") %>%
        mutate(var = if_else(var %in% c("d", "varD", "g", "varG"),
                             paste0(var, ".fu", gsub(".*(?=_)", "", followUp, perl = T)),
                             var),
               followUp = gsub("_.*", "", followUp)) %>%
        pivot_wider(names_from = "var",
                    values_from = "value") %>%
        drop_na(time) %>%
        mutate(groupByMeasure = effectLabel,
               effectLabel = paste0(effectLabel, " (", time, " weeks)"),
               effectLabelNew = paste0(effectLabelNew, " (", time, " weeks)"),
               effectLabelAlt = paste0(effectLabelAlt, " (", time, " weeks)"),
               followUpLength = case_when(time < 3 ~ "Less than 3 weeks",
                                          time <= 12 ~ "Between 3 and 12 weeks",
                                          time > 12 ~ "Greater than 12 weeks"))
```

# Characteristics of included studies

```{r}
vars <- names(df)[3:35]

for(var in vars) {
  
  print("--------------------------------------------------")
  print(var)
  print("")
  
  if(class(df[[var]]) == "numeric") {
    
    print(paste0("Mean: ", round(mean(df[[var]], na.rm = T), 2)))
    print(paste0("Median: ", round(median(df[[var]], na.rm = T), 2)))
    
  } else {
    
    df %>%
      count(!!rlang::sym(var)) %>%
      print()
    
  }
  
  print("")
  print("")
  print("")
  
}
```

# Meta-analysis

Mixed-effects model with robust variance estimation using `metafor`

## Post-test effects

### Intercept-only model

#### Using post-test data only

```{r}
model.post.IO_t2 <- rma.mv(yi = g.post_t2 ~ 1,
                           V = varG.post_t2,
                           random = ~ 1 | studyID,
                           data = df) %>%
  robust(cluster = df$studyID)

print(model.post.IO_t2)
```

#### Using pre- and post-test data

```{r}
model.post.IO_t1t2 <- rma.mv(yi = g.post_t1t2 ~ 1,
                             V = varG.post_t1t2,
                             random = ~ 1 | studyID,
                             data = df) %>%
  robust(cluster = df$studyID)

print(model.post.IO_t1t2)
```

### Forest plot: 

```{r}
out <- df %>%
  mutate(est = g.post_t2,
         lower = g.post_t2 - (1.96 * sqrt(varG.post_t2)),
         upper = g.post_t2 + (1.96 * sqrt(varG.post_t2)),
         n = t.post.n + c.post.n,
         article = factor(article)) %>%
  select(article, effectLabelAlt, est, lower, upper, n)

out <- rbind(out,
             tibble(
               article = "Overall Estimate",
               effectLabelAlt = "Overall Estimate",
               est = NA, 
               lower = NA, 
               upper = NA, 
               n = NA
             ))

diamond <- data.frame(x = c(model.post.IO_t2$ci.ub, 
                            model.post.IO_t2$b[1,1], 
                            model.post.IO_t2$ci.lb, 
                            model.post.IO_t2$b[1,1]), 
                      y = c(1, 1.5, 1, .5),
                      article = "Overall Estimate")

ggplot(out, 
       aes(y = effectLabelAlt, 
           x = est,
           xmin = lower,
           xmax = upper)) +
  geom_point(aes(size = n), 
             shape = "square") +
  geom_errorbarh() +
  scale_y_discrete(name = NULL, 
                   limits = rev) +
  scale_x_continuous(name = "Hedge's g") +
  scale_size_continuous(range = c(1, 4)) +
  facet_grid(article ~ .,
             scales = "free_y",
             space = "free_y",
             switch = "y") +
  theme(strip.placement = "outside",
        strip.text.y.left = element_text(angle = 0),
        legend.position = "none",
        panel.background = element_rect(fill = NA),
        axis.line.x = element_line(size = .5, color = "black"),
        axis.ticks.y.left = element_blank()) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_polygon(data = diamond, 
               aes(x = x, y = y),
               inherit.aes = F,
               fill = "black")
```

### Risk of bias assessment

We will assess publication bias with a funnel plot and Egger's test. Since we cannot use `regtest` with `rma.mv`, we will instead conduct it manually within the framework of our multilevel model. We do this by regressing each effect's normalized effect estimate (estimate divided by its standard error) against precision (reciprocal of the standard error of the estimate) and testing the significance of the intercept.

```{r}
df <- df %>%
  mutate(seG.post_t2 = sqrt(varG.post_t2),
         gStd.post_t2 = g.post_t2 / seG.post_t2,
         precision.post_t2 = 1 / seG.post_t2)

rma.mv(yi = gStd.post_t2 ~ 1 + precision.post_t2,
       V = varG.post_t2,
       random = ~ 1 | studyID,
       data = df) %>%
  robust(cluster = df$studyID)
```

In addition, we tested the following moderators, included in table below: `blindAssign`, `manual`, `preTraining`, `supervision`, `studyCompletion.post`

```{r}
est = model.post.IO_t2$b[[1]]
seMax = max(df$seG.post_t2)

#line 1
a1 = c(est, 0)
b1 = c(est + 1.96 * seMax, seMax)
m1 = (b1[2] - a1[2]) / (b1[1] - a1[1])
b1 = -est * m1

#line 2
a2 = c(est, 0)
b2 = c(est - 1.96 * seMax, seMax)
m2 = (b2[2] - a2[2]) / (b2[1] - a2[1])
b2 = -est * m2

df %>%
  ggplot(aes(g.post_t2, seG.post_t2)) +
  geom_point() +
  scale_x_continuous(name = "Hedge's g", limits = c(est - 1, est + 1)) +
  scale_y_reverse(name = "Standard Error", limits = c(.325, 0), expand = c(0, 0)) +
  geom_vline(xintercept = est) +
  geom_abline(intercept = b1, slope = m1, linetype = 2) +
  geom_abline(intercept = b2, slope = m2, linetype = 2) +
  theme_classic()
```

```{r}
df$n = df$c.pre.n + df$t.pre.n

df %>%
  ggplot(aes(g.post_t2, n)) +
  geom_point() +
  scale_x_continuous(name = "Hedge's g", limits = c(est - 1, est + 1)) +
  scale_y_continuous(name = "n") +
  geom_vline(xintercept = est) +
  theme_classic()
```

### Moderation tests

Moderation tests for each of the following variables (pre-registered), including subgroups with n >= 3 studies (not effect sizes).

See for minimum n/group in moderator test (they suggest 4, we use 3): Fu, R., Gartlehner, G., Grant, M., Shamliyan, T., Sedrakyan, A., Wilt, T. J., ... & Trikalinos, T. A. (2011). Conducting quantitative synthesis when comparing medical interventions: AHRQ and the Effective Health Care Program. Journal of clinical epidemiology, 64(11), 1187-1197. <https://www.researchgate.net/post/Does-anyone-have-any-citations-supporting-a-specific-number-of-studies-eg-3-or-more-per-subgroup-in-a-categorical-moderator-analysis>

To decompose effects of effect-level and study-level moderation, see page 13: <https://cran.r-project.org/web/packages/robumeta/vignettes/robumetaVignette.pdf>

To use random slopes for moderators: `random = ~ mod | studyID, struct="GEN"` <https://stats.stackexchange.com/questions/526230/in-a-multi-level-meta-analysis-are-moderator-variables-random-or-fixed-effects?noredirect=1#comment969486_526230>

Pre-registered moderators: `year`, `pubStatus`, `country`, `sampleSource`, `meanAge`, `pctFemale`, `pctNonwhite`, `interventionType`, `riskScreen`, `outcome`, `controlCond`, `deliveryFormat`, `target`, `provider`, `supervision`, `length.minutes`, `length.sessions`, `length.days`, `trtCompletion`, `studyCompletion.post`

Added after pre-registration: `interventionParticipant`, `meanType`, `randUnits`

Moderators tested for quality assessment: `blindAssign`, `manual`, `preTraining`, `supervision`, `studyCompletion.post`

Dropped due to zero variation:

- `pctSexualMinority`: All `NA`
- `respondent`: All "Youth-reported"
- `manual`: All "Treatment manual used" or `NA`
- `preTraining`: All "Present" or `NA`

Dropped because only one category had >= 3 studies:

- `pubStatus`: Only 1 not "Peer-reviewed"
- `sampleSource`: Only 1 not "Community sample"
- `delieryFormat`: Only 2 not "Group" 
- `target`: Only 2 not "Body dissatisfaction"
- `interventionParticipant`: Only 2 not "Youth"
- `meanType`: Only 1 not "Unadjusted"
- `blindAssign`: Only 1 not "No blind assignment"

Collapsed categories to allow for analysis: _(Neither of these ping as significant; maybe don't collapse categories)_

- `controlCond`: "Psychoeducation" (2) and "Other" (1) combined into "Active control"
- `provider`: "Self-administered" (1) and "Lay-administered" (2) combined into "Non-clinician-administered"

```{r}
moderators <- c(
  #Study characteristics
  "year", "pubStatus", "country", 
  #Sampling
  "sampleSource", "interventionType", "riskScreen", 
  #Population
  "meanAge", "pctFemale", "pctNonwhite", 
  #Intervention
  "target", "provider", "interventionParticipant", "deliveryFormat", "length.minutes", "length.sessions", "length.days", "trtCompletion", "controlCond",
  #Outcome
  "outcome",
  #Quality assessment
  "blindAssign", "manual", "supervision", "studyCompletion.post", "randUnits")

df <- df %>%
  mutate(controlCond = if_else(controlCond %in% c("Psychoeducation", "Other"), "Active control", controlCond),
         provider = if_else(provider %in% c("Lay-administered", "Self-administered"), "Non-clinician-administered", provider))

out <- tibble(
  moderator = NA_character_,
  level = NA_character_,
  nStudies = NA_real_,
  nES = NA_real_,
  subgroupEst = NA_real_,
  subgroupP = NA_real_,
  coef = NA_real_,
  overallStat = NA_real_,
  overallP = NA_real_
)

for(moderator in moderators) {
  
  class <- class(df[[moderator]])
  
  if(class == "numeric") {
    
    model <- rma.mv(as.formula(paste0("yi = g.post_t2 ~ 1 + ", moderator)),
                  V = varG.post_t2,
                  random = ~ 1 | studyID,
                  data = df[!is.na(df[[moderator]]),]) %>%
      robust(cluster = df$studyID[!is.na(df[[moderator]])])
    
    modTest <- anova.rma(model)
    
    coef = coef(model)[2]
    
    test = modTest$test
    dfs = modTest$dfs
    stat = model$zval[2]
    p = modTest$QMp
    
    nStudies = length(unique(df$studyID[!is.na(df[[moderator]])]))
    nES = sum(!is.na(df[[moderator]]))
    
    out <- rbind(out, tibble(
      moderator = moderator,
      level = NA,
      nStudies = nStudies,
      nES = nES,
      subgroupEst = NA,
      subgroupP = NA,
      coef = coef,
      overallStat = paste0(test, "(", dfs, ") = ", round(stat, 3)),
      overallP = p
    ))
  
  } else if(class == "character") {
    
    values <- df[[moderator]][!duplicated(df$studyID)]
    table <- table(values)
    levels <- names(table)[table >= 3]
  
    if(length(levels) > 1) {
      
      model <- rma.mv(as.formula(paste0("yi = g.post_t2 ~ 1 + ", moderator)),
                    V = varG.post_t2,
                    random = ~ 1 | studyID,
                    data = df[df[[moderator]] %in% levels,]) %>%
        robust(cluster = df$studyID[df[[moderator]] %in% levels])
    
      modTest <- anova.rma(model)
    
      test = "F"
      dfs = paste0(modTest$m, ", ", modTest$dfs)
      stat = modTest$QM
      p = modTest$QMp
      
      out <- rbind(out, tibble(
        moderator = moderator,
        level = NA,
        nStudies = NA,
        nES = NA,
        subgroupEst = NA,
        subgroupP = NA,
        coef = NA,
        overallStat = paste0(test, "(", dfs, ") = ", round(stat, 3)),
        overallP = p
      ))
        
      for(level in levels) {
        
        nStudies = length(unique(df$studyID[df[[moderator]] %in% level]))
        nES = sum(df[[moderator]] %in% level)
        
        model <- rma.mv(yi = g.post_t2 ~ 1,
                        V = varG.post_t2,
                        random = ~ 1 | studyID,
                        data = df[df[[moderator]] %in% level,]) %>%
          robust(cluster = df$studyID[df[[moderator]] %in% level])
        
        subgroupEst = model$b[[1]]
        subgroupP = model$pval
          
        out <- rbind(out, tibble(
          moderator = moderator,
          level = level,
          nStudies = nStudies,
          nES = nES,
          subgroupEst = subgroupEst,
          subgroupP = subgroupP,
          coef = NA,
          overallStat = NA,
          overallP = NA
        ))
        
      }
      
    }
    
  } else {
    
    stop("Moderator not numeric or character")
    
  }
  
}

out <- out[-1,]

out <- out %>%
  mutate(moderator = recode(moderator,
                            "year" = "Year",
                            "country" = "Country", 
                            "interventionType" = "Intervention Type",
                            "riskScreen" = "Risk Screening",
                            "meanAge" = "Mean Age",
                            "pctFemale" = "Percent Female",
                            "pctNonwhite" = "Percent Non-white",
                            "provider" = "Provider",
                            "length.minutes" = "Length (Minutes)",
                            "length.sessions" = "Length (Sessions)",
                            "length.days" = "Length (Days)",
                            "trtCompletion" = "Treatment Completion Rate",
                            "controlCond" = "Control Condition",
                            "outcome" = "Outcome",
                            "supervision" = "Facilitator Supervision",
                            "studyCompletion.post" = "Post-test Data Availability",
                            "randUnits" = "Randomized Units"))

write.csv(out, "C:\\Users\\isaac\\Google Drive\\Research\\Projects\\Body Dissatisfaction Meta-Analysis\\BD-depression-meta-analysis\\Outputs\\Moderator Table - Post-Treatment.csv")

options(knitr.kable.NA = "")
kable(out, digits = 3) %>%
  kable_paper() %>%
  add_header_above(c(" " = 4, "Subgroup Analysis" = 3, "Moderator Test" = 2)) %>%
  column_spec(1, bold = T) %>%
  collapse_rows(columns = 1, valign = "top")
```

_End of post-test effects analysis._

## Follow-up effects

### Intercept-only model

#### Using follow-up data only

```{r}
model.fu.IO_t2 <- rma.mv(yi = g.fu_t2 ~ 1,
                         V = varG.fu_t2,
                         random = ~ 1 | studyID, #Same as 1 | studyID/groupByMeasure
                         data = df.fu) %>%
  robust(cluster = df.fu$studyID)

print(model.fu.IO_t2)
```

#### Using pre-test and follow-up data

```{r}
model.fu.IO_t1t2 <- rma.mv(yi = g.fu_t1t2 ~ 1,
                           V = varG.fu_t1t2,
                           random = ~ 1 | studyID,
                           data = df.fu) %>%
  robust(cluster = df.fu$studyID)

print(model.fu.IO_t1t2)
```

### Forest plot

```{r}
out <- df.fu %>%
  mutate(est = g.fu_t2,
         lower = g.fu_t2 - (1.96 * sqrt(varG.fu_t2)),
         upper = g.fu_t2 + (1.96 * sqrt(varG.fu_t2)),
         n = nt + nc,
         article = factor(article)) %>%
  select(article, effectLabelAlt, est, lower, upper, n)

out <- rbind(out,
             tibble(
               article = "Overall Estimate",
               effectLabelAlt = "Overall Estimate",
               est = NA, 
               lower = NA, 
               upper = NA, 
               n = NA
             ))

diamond <- data.frame(x = c(model.fu.IO_t2$ci.ub, 
                            model.fu.IO_t2$b[1,1], 
                            model.fu.IO_t2$ci.lb, 
                            model.fu.IO_t2$b[1,1]), 
                      y = c(1, 1.5, 1, .5),
                      article = "Overall Estimate")

ggplot(out, 
       aes(y = effectLabelAlt, 
           x = est,
           xmin = lower,
           xmax = upper)) +
  geom_point(aes(size = n), 
             shape = "square") +
  geom_errorbarh() +
  scale_y_discrete(name = NULL, 
                   limits = rev) + 
  scale_x_continuous(name = "Hedge's g") +
  scale_size_continuous(range = c(1, 4)) +
  facet_grid(article ~ .,
             scales = "free_y",
             space = "free_y",
             switch = "y") +
  theme(strip.placement = "outside",
        strip.text.y.left = element_text(angle = 0),
        legend.position = "none",
        panel.background = element_rect(fill = NA),
        axis.line.x = element_line(size = .5, color = "black"),
        axis.ticks.y.left = element_blank()) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_polygon(data = diamond, 
               aes(x = x, y = y),
               inherit.aes = F,
               fill = "black")
```

### Risk of bias assessment

We will assess publication bias with a funnel plot and Egger's test. Since we cannot use `regtest` with `rma.mv`, we will instead conduct it manually within the framework of our multilevel model. We do this by regressing each effect's normalized effect estimate (estimate divided by its standard error) against precision (reciprocal of the standard error of the estimate) and testing the significance of the intercept.

```{r}
df.fu <- df.fu %>%
  mutate(seG.fu_t2 = sqrt(varG.fu_t2),
         gStd.fu_t2 = g.fu_t2 / seG.fu_t2,
         precision.fu_t2 = 1 / seG.fu_t2)

rma.mv(yi = gStd.fu_t2 ~ 1 + precision.fu_t2,
       V = varG.fu_t2,
       random = ~ 1 | studyID,
       data = df.fu) %>%
  robust(cluster = df.fu$studyID)
```

In addition, we tested the following moderators, included in table below: `blindAssign`, `manual`, `preTraining`, `supervision`, `studyCompletion.post`

```{r}
est = model.fu.IO_t2$b[[1]]
seMax = max(df.fu$seG.fu_t2)

#line 1
a1 = c(est, 0)
b1 = c(est + 1.96 * seMax, seMax)
m1 = (b1[2] - a1[2]) / (b1[1] - a1[1])
b1 = -est * m1

#line 2
a2 = c(est, 0)
b2 = c(est - 1.96 * seMax, seMax)
m2 = (b2[2] - a2[2]) / (b2[1] - a2[1])
b2 = -est * m2

df.fu %>%
  ggplot(aes(g.fu_t2, seG.fu_t2)) +
  geom_point() +
  scale_x_continuous(name = "Hedge's g", limits = c(est - 1, est + 1)) +
  scale_y_reverse(name = "Standard Error", limits = c(.325, 0), expand = c(0, 0)) +
  geom_vline(xintercept = est) +
  geom_abline(intercept = b1, slope = m1, linetype = 2) +
  geom_abline(intercept = b2, slope = m2, linetype = 2) +
  theme_classic()
```

```{r}
df.fu$n = df.fu$nc + df.fu$nt

df.fu %>%
  ggplot(aes(g.fu_t2, n)) +
  geom_point() +
  scale_x_continuous(name = "Hedge's g", limits = c(est - 1, est + 1)) +
  scale_y_continuous(name = "n") +
  geom_vline(xintercept = est) +
  theme_classic()
```

### Moderation tests

Moderation tests for each of the following variables (pre-registered) with n >= 3 studies (not effect sizes) per group.

Pre-registered moderators: `year`, `pubStatus`, `country`, `sampleSource`, `meanAge`, `pctFemale`, `pctNonwhite`, `interventionType`, `riskScreen`, `outcome`, `controlCond`, `deliveryFormat`, `target`, `provider`, `supervision`, `length.minutes`, `length.sessions`, `length.days`, `trtCompletion`, `studyCompletion`, `followUpLength`

Added after pre-registration: `interventionParticipant`, `meanType`, `randUnits`

Moderators tested for quality assessment: `blindAssign`, `manual`, `preTraining`, `supervision`, `studyCompletion`

Dropped due to zero variation:

- `pctSexualMinority`: All `NA`
- `respondent`: All "Youth-reported"
- `manual`: All "Treatment manual used" or `NA`
- `preTraining`: All "Present" or `NA`

Dropped because only one category had >= 3 studies:

- `pubStatus`: Only 1 not "Peer-reviewed"
- `sampleSource`: Only 1 not "Community sample"
- `delieryFormat`: Only 1 not "Group" 
- `target`: Only 2 not "Body dissatisfaction"
- `provider`: Only 2 not "Clinician-administered"
- `interventionParticipant`: Only 2 not "Youth"
- `meanType`: Only 1 not "Unadjusted"
- `blindAssign`: Only 1 not "No blind assignment"

Collapsed categories to allow for analysis: _(Not significant; maybe don't collapse categories)_

- `controlCond`: "Psychoeducation" (2) and "Other" (1) combined into "Active control"

```{r}
moderators <- c(
  #Study characteristics
  "year", "pubStatus", "country", 
  #Sampling
  "sampleSource", "interventionType", "riskScreen", 
  #Population
  "meanAge", "pctFemale", "pctNonwhite", 
  #Intervention
  "target", "provider", "interventionParticipant", "deliveryFormat", "length.minutes", "length.sessions", "length.days", "trtCompletion", "controlCond",
  #Outcome
  "outcome", "followUpLength",
  #Quality assessment
  "blindAssign", "manual", "supervision", "studyCompletion", "randUnits")

df.fu <- df.fu %>%
  mutate(controlCond = if_else(controlCond %in% c("Psychoeducation", "Other"), "Active control", controlCond))

out <- tibble(
  moderator = NA_character_,
  level = NA_character_,
  nStudies = NA_real_,
  nES = NA_real_,
  subgroupEst = NA_real_,
  subgroupP = NA_real_,
  coef = NA_real_,
  overallStat = NA_real_,
  overallP = NA_real_
)

for(moderator in moderators) {
  
  class <- class(df.fu[[moderator]])
  
  if(class == "numeric") {
    
    model <- rma.mv(as.formula(paste0("yi = g.fu_t2 ~ 1 + ", moderator)),
                  V = varG.fu_t2,
                  random = ~ 1 | studyID,
                  data = df.fu[!is.na(df.fu[[moderator]]),]) %>%
      robust(cluster = df.fu$studyID[!is.na(df.fu[[moderator]])])
    
    modTest <- anova.rma(model)
    
    coef = coef(model)[2]
    
    test = modTest$test
    dfs = modTest$dfs
    stat = model$zval[2]
    p = modTest$QMp
    
    nStudies = length(unique(df.fu$studyID[!is.na(df.fu[[moderator]])]))
    nES = sum(!is.na(df.fu[[moderator]]))
    
    out <- rbind(out, tibble(
      moderator = moderator,
      level = NA,
      nStudies = nStudies,
      nES = nES,
      subgroupEst = NA,
      subgroupP = NA,
      coef = coef,
      overallStat = paste0(test, "(", dfs, ") = ", round(stat, 3)),
      overallP = p
    ))
  
  } else if(class == "character") {
    
    values <- df.fu[[moderator]][!duplicated(df.fu$studyID)]
    table <- table(values)
    levels <- names(table)[table >= 3]
  
    if(length(levels) > 1) {
      
      model <- rma.mv(as.formula(paste0("yi = g.fu_t2 ~ 1 + ", moderator)),
                    V = varG.fu_t2,
                    random = ~ 1 | studyID,
                    data = df.fu[df.fu[[moderator]] %in% levels,]) %>%
        robust(cluster = df.fu$studyID[df.fu[[moderator]] %in% levels])
    
      modTest <- anova.rma(model)
    
      test = "F"
      dfs = paste0(modTest$m, ", ", modTest$dfs)
      stat = modTest$QM
      p = modTest$QMp
      
      out <- rbind(out, tibble(
        moderator = moderator,
        level = NA,
        nStudies = NA,
        nES = NA,
        subgroupEst = NA,
        subgroupP = NA,
        coef = NA,
        overallStat = paste0(test, "(", dfs, ") = ", round(stat, 3)),
        overallP = p
      ))
        
      for(level in levels) {
        
        nStudies = length(unique(df.fu$studyID[df.fu[[moderator]] %in% level]))
        nES = sum(df.fu[[moderator]] %in% level)
        
        model <- rma.mv(yi = g.fu_t2 ~ 1,
                        V = varG.fu_t2,
                        random = ~ 1 | studyID,
                        data = df.fu[df.fu[[moderator]] %in% level,]) %>%
          robust(cluster = df.fu$studyID[df.fu[[moderator]] %in% level])
        
        subgroupEst = model$b[[1]]
        subgroupP = model$pval
          
        out <- rbind(out, tibble(
          moderator = moderator,
          level = level,
          nStudies = nStudies,
          nES = nES,
          subgroupEst = subgroupEst,
          subgroupP = subgroupP,
          coef = NA,
          overallStat = NA,
          overallP = NA
        ))
        
      }
      
    }
    
  } else {
    
    stop("Moderator not numeric or character")
    
  }
  
}

out <- out[-1,]

out <- out %>%
  mutate(moderator = recode(moderator,
                            "year" = "Year",
                            "country" = "Country", 
                            "interventionType" = "Intervention Type",
                            "riskScreen" = "Risk Screening",
                            "meanAge" = "Mean Age",
                            "pctFemale" = "Percent Female",
                            "pctNonwhite" = "Percent Non-white",
                            "provider" = "Provider",
                            "length.minutes" = "Length (Minutes)",
                            "length.sessions" = "Length (Sessions)",
                            "length.days" = "Length (Days)",
                            "trtCompletion" = "Treatment Completion Rate",
                            "controlCond" = "Control Condition",
                            "outcome" = "Outcome",
                            "followUpLength" = "Follow-up Length",
                            "supervision" = "Facilitator Supervision",
                            "studyCompletion.post" = "Post-test Data Availability",
                            "randUnits" = "Randomized Units"))

write.csv(out, "C:\\Users\\isaac\\Google Drive\\Research\\Projects\\Body Dissatisfaction Meta-Analysis\\BD-depression-meta-analysis\\Outputs\\Moderator Table - Follow-Up.csv")

options(knitr.kable.NA = "")
kable(out, digits = 3) %>%
  kable_paper() %>%
  add_header_above(c(" " = 4, "Subgroup Analysis" = 3, "Moderator Test" = 2)) %>%
  column_spec(1, bold = T) %>%
  collapse_rows(columns = 1, valign = "top")
```

_End of follow-up effects analysis._
